<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>scrapfly.scrape_config API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>scrapfly.scrape_config</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import json
from base64 import b64encode
from os import getpid
from socket import gethostname
from threading import currentThread
from typing import Optional, List, Dict
from urllib.parse import urlencode

from requests.structures import CaseInsensitiveDict


class ScrapeConfigError(Exception):
    pass


class ScrapeConfig:

    def __init__(
            self,
            url: str,
            retry: bool = True,
            method: str = &#39;GET&#39;,
            country: Optional[str] = &#39;DE&#39;,
            render_js: bool = False,
            cache: bool = False,
            cache_clear:bool = False,
            ssl:bool = False,
            dns:bool = False,
            asp:bool = False,
            cache_ttl:Optional[int] = None,
            session: Optional[str] = None,
            debug: Optional[bool] = False,
            tags: Optional[List[str]] = None,
            correlation_id: Optional[str] = None,
            cookies: Optional[Dict] = None,
            body: Optional[str] = None,
            data: Optional[Dict] = None,
            headers: Optional[Dict[str, str]] = None,
            graphql: Optional[str] = None,
            js: str = None,
            rendering_wait: int = None,
            screenshots:Optional[Dict]=None,
            raise_on_upstream_error:bool=True
    ):
        assert(type(url) is str)

        self.cookies = CaseInsensitiveDict(cookies or {})
        self.headers = CaseInsensitiveDict(headers or {})
        self.url = url
        self.retry = retry
        self.method = method
        self.country = country
        self.render_js = render_js
        self.cache = cache
        self.cache_clear = cache_clear
        self.asp = asp
        self.session = session
        self.debug = debug
        self.cache_ttl = cache_ttl
        self.tags = tags
        self.correlation_id = correlation_id
        self.body = body
        self.data = data
        self.graphql = graphql
        self.js = js
        self.rendering_wait = rendering_wait
        self.raise_on_upstream_error = raise_on_upstream_error
        self.screenshots = screenshots
        self.key = None
        self.dns = dns
        self.ssl = ssl

        if cookies:
            _cookies = []

            for name, value in cookies.items():
                _cookies.append(name + &#39;=&#39; + value)

            if &#39;cookie&#39; in self.headers:
                if self.headers[&#39;cookie&#39;][-1] != &#39;;&#39;:
                    self.headers[&#39;cookie&#39;] += &#39;;&#39;
                else:
                    self.headers[&#39;cookie&#39;] = &#39;&#39;

            self.headers[&#39;cookie&#39;] += &#39;; &#39;.join(_cookies)

        if self.body and self.data:
            raise ScrapeConfigError(&#39;You cannot pass both parameters body and data. You must choose&#39;)

        if method in [&#39;POST&#39;, &#39;PUT&#39;, &#39;PATCH&#39;] and self.body is None and self.data is not None:
            print(self.headers)
            if &#39;content-type&#39; not in self.headers:
                self.headers[&#39;content-type&#39;] = &#39;application/x-www-form-urlencoded&#39;
                self.body = urlencode(data)
            else:
                if self.headers[&#39;content-type&#39;] == &#39;application/json&#39;:
                    self.body = json.dumps(data)
                elif &#39;application/x-www-form-urlencoded&#39; == self.headers[&#39;content-type&#39;]:
                    self.body = urlencode(data)
                else:
                    raise ScrapeConfigError(&#39;Content Type %s not support, use body parameter to pass pre encoded body according to your content type&#39; % self.headers[&#39;content-type&#39;])

    def _bool_to_http(self, _bool:bool) -&gt; str:
        return &#39;true&#39; if _bool is True else &#39;false&#39;

    def generate_distributed_correlation_id(self):
        self.correlation_id = abs(hash(&#39;-&#39;.join([gethostname(), str(getpid()), str(currentThread().ident)])))

    def to_api_params(self, key:str) -&gt; Dict:
        params = {
            &#39;key&#39;: self.key if self.key is not None else key,
            &#39;url&#39;: self.url,
            &#39;country&#39;: self.country,
        }

        for name, value in self.headers.items():
            params[&#39;headers[%s]&#39; % name] = value

        if self.render_js is True:
            params[&#39;render_js&#39;] = self._bool_to_http(self.render_js)

        if self.asp is True:
            params[&#39;asp&#39;] = self._bool_to_http(self.asp)

        if self.retry is False:
            params[&#39;retry&#39;] = self._bool_to_http(self.retry)

        if self.cache is True:
            params[&#39;cache&#39;] = self._bool_to_http(self.cache)

        if self.dns is True:
            params[&#39;dns&#39;] = self._bool_to_http(self.dns)

        if self.ssl is True:
            params[&#39;ssl&#39;] = self._bool_to_http(self.ssl)

        if self.tags:
            params[&#39;tags&#39;] = &#39;,&#39;.join(self.tags)

        if self.correlation_id:
            params[&#39;correlation_id&#39;] = self.correlation_id

        if self.screenshots is not None:
            for name, element in self.screenshots.items():
                params[&#39;screenshots[%s]&#39; % name] = element

        if self.session:
            params[&#39;session&#39;] = self.session

        if self.debug is True:
            params[&#39;debug&#39;] = self._bool_to_http(self.debug)

        if self.cache_clear is True:
            params[&#39;cache_clear&#39;] = self._bool_to_http(self.cache_clear)

        if self.cache_ttl is not None:
            params[&#39;cache_ttl&#39;] = self.cache_ttl

        if self.graphql:
            params[&#39;graphql&#39;] = self.graphql

        if self.js:
            params[&#39;js&#39;] = b64encode(self.js.encode(&#39;utf-8&#39;)).decode(&#39;utf-8&#39;)

        if self.rendering_wait:
            params[&#39;rendering_wait&#39;] = self.rendering_wait

        return params</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="scrapfly.scrape_config.ScrapeConfig"><code class="flex name class">
<span>class <span class="ident">ScrapeConfig</span></span>
<span>(</span><span>url: str, retry: bool = True, method: str = 'GET', country: Union[str, NoneType] = 'DE', render_js: bool = False, cache: bool = False, cache_clear: bool = False, ssl: bool = False, dns: bool = False, asp: bool = False, cache_ttl: Union[int, NoneType] = None, session: Union[str, NoneType] = None, debug: Union[bool, NoneType] = False, tags: Union[List[str], NoneType] = None, correlation_id: Union[str, NoneType] = None, cookies: Union[Dict, NoneType] = None, body: Union[str, NoneType] = None, data: Union[Dict, NoneType] = None, headers: Union[Dict[str, str], NoneType] = None, graphql: Union[str, NoneType] = None, js: str = None, rendering_wait: int = None, screenshots: Union[Dict, NoneType] = None, raise_on_upstream_error: bool = True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScrapeConfig:

    def __init__(
            self,
            url: str,
            retry: bool = True,
            method: str = &#39;GET&#39;,
            country: Optional[str] = &#39;DE&#39;,
            render_js: bool = False,
            cache: bool = False,
            cache_clear:bool = False,
            ssl:bool = False,
            dns:bool = False,
            asp:bool = False,
            cache_ttl:Optional[int] = None,
            session: Optional[str] = None,
            debug: Optional[bool] = False,
            tags: Optional[List[str]] = None,
            correlation_id: Optional[str] = None,
            cookies: Optional[Dict] = None,
            body: Optional[str] = None,
            data: Optional[Dict] = None,
            headers: Optional[Dict[str, str]] = None,
            graphql: Optional[str] = None,
            js: str = None,
            rendering_wait: int = None,
            screenshots:Optional[Dict]=None,
            raise_on_upstream_error:bool=True
    ):
        assert(type(url) is str)

        self.cookies = CaseInsensitiveDict(cookies or {})
        self.headers = CaseInsensitiveDict(headers or {})
        self.url = url
        self.retry = retry
        self.method = method
        self.country = country
        self.render_js = render_js
        self.cache = cache
        self.cache_clear = cache_clear
        self.asp = asp
        self.session = session
        self.debug = debug
        self.cache_ttl = cache_ttl
        self.tags = tags
        self.correlation_id = correlation_id
        self.body = body
        self.data = data
        self.graphql = graphql
        self.js = js
        self.rendering_wait = rendering_wait
        self.raise_on_upstream_error = raise_on_upstream_error
        self.screenshots = screenshots
        self.key = None
        self.dns = dns
        self.ssl = ssl

        if cookies:
            _cookies = []

            for name, value in cookies.items():
                _cookies.append(name + &#39;=&#39; + value)

            if &#39;cookie&#39; in self.headers:
                if self.headers[&#39;cookie&#39;][-1] != &#39;;&#39;:
                    self.headers[&#39;cookie&#39;] += &#39;;&#39;
                else:
                    self.headers[&#39;cookie&#39;] = &#39;&#39;

            self.headers[&#39;cookie&#39;] += &#39;; &#39;.join(_cookies)

        if self.body and self.data:
            raise ScrapeConfigError(&#39;You cannot pass both parameters body and data. You must choose&#39;)

        if method in [&#39;POST&#39;, &#39;PUT&#39;, &#39;PATCH&#39;] and self.body is None and self.data is not None:
            print(self.headers)
            if &#39;content-type&#39; not in self.headers:
                self.headers[&#39;content-type&#39;] = &#39;application/x-www-form-urlencoded&#39;
                self.body = urlencode(data)
            else:
                if self.headers[&#39;content-type&#39;] == &#39;application/json&#39;:
                    self.body = json.dumps(data)
                elif &#39;application/x-www-form-urlencoded&#39; == self.headers[&#39;content-type&#39;]:
                    self.body = urlencode(data)
                else:
                    raise ScrapeConfigError(&#39;Content Type %s not support, use body parameter to pass pre encoded body according to your content type&#39; % self.headers[&#39;content-type&#39;])

    def _bool_to_http(self, _bool:bool) -&gt; str:
        return &#39;true&#39; if _bool is True else &#39;false&#39;

    def generate_distributed_correlation_id(self):
        self.correlation_id = abs(hash(&#39;-&#39;.join([gethostname(), str(getpid()), str(currentThread().ident)])))

    def to_api_params(self, key:str) -&gt; Dict:
        params = {
            &#39;key&#39;: self.key if self.key is not None else key,
            &#39;url&#39;: self.url,
            &#39;country&#39;: self.country,
        }

        for name, value in self.headers.items():
            params[&#39;headers[%s]&#39; % name] = value

        if self.render_js is True:
            params[&#39;render_js&#39;] = self._bool_to_http(self.render_js)

        if self.asp is True:
            params[&#39;asp&#39;] = self._bool_to_http(self.asp)

        if self.retry is False:
            params[&#39;retry&#39;] = self._bool_to_http(self.retry)

        if self.cache is True:
            params[&#39;cache&#39;] = self._bool_to_http(self.cache)

        if self.dns is True:
            params[&#39;dns&#39;] = self._bool_to_http(self.dns)

        if self.ssl is True:
            params[&#39;ssl&#39;] = self._bool_to_http(self.ssl)

        if self.tags:
            params[&#39;tags&#39;] = &#39;,&#39;.join(self.tags)

        if self.correlation_id:
            params[&#39;correlation_id&#39;] = self.correlation_id

        if self.screenshots is not None:
            for name, element in self.screenshots.items():
                params[&#39;screenshots[%s]&#39; % name] = element

        if self.session:
            params[&#39;session&#39;] = self.session

        if self.debug is True:
            params[&#39;debug&#39;] = self._bool_to_http(self.debug)

        if self.cache_clear is True:
            params[&#39;cache_clear&#39;] = self._bool_to_http(self.cache_clear)

        if self.cache_ttl is not None:
            params[&#39;cache_ttl&#39;] = self.cache_ttl

        if self.graphql:
            params[&#39;graphql&#39;] = self.graphql

        if self.js:
            params[&#39;js&#39;] = b64encode(self.js.encode(&#39;utf-8&#39;)).decode(&#39;utf-8&#39;)

        if self.rendering_wait:
            params[&#39;rendering_wait&#39;] = self.rendering_wait

        return params</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="scrapfly.scrape_config.ScrapeConfig.generate_distributed_correlation_id"><code class="name flex">
<span>def <span class="ident">generate_distributed_correlation_id</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_distributed_correlation_id(self):
    self.correlation_id = abs(hash(&#39;-&#39;.join([gethostname(), str(getpid()), str(currentThread().ident)])))</code></pre>
</details>
</dd>
<dt id="scrapfly.scrape_config.ScrapeConfig.to_api_params"><code class="name flex">
<span>def <span class="ident">to_api_params</span></span>(<span>self, key: str) ‑> Dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_api_params(self, key:str) -&gt; Dict:
    params = {
        &#39;key&#39;: self.key if self.key is not None else key,
        &#39;url&#39;: self.url,
        &#39;country&#39;: self.country,
    }

    for name, value in self.headers.items():
        params[&#39;headers[%s]&#39; % name] = value

    if self.render_js is True:
        params[&#39;render_js&#39;] = self._bool_to_http(self.render_js)

    if self.asp is True:
        params[&#39;asp&#39;] = self._bool_to_http(self.asp)

    if self.retry is False:
        params[&#39;retry&#39;] = self._bool_to_http(self.retry)

    if self.cache is True:
        params[&#39;cache&#39;] = self._bool_to_http(self.cache)

    if self.dns is True:
        params[&#39;dns&#39;] = self._bool_to_http(self.dns)

    if self.ssl is True:
        params[&#39;ssl&#39;] = self._bool_to_http(self.ssl)

    if self.tags:
        params[&#39;tags&#39;] = &#39;,&#39;.join(self.tags)

    if self.correlation_id:
        params[&#39;correlation_id&#39;] = self.correlation_id

    if self.screenshots is not None:
        for name, element in self.screenshots.items():
            params[&#39;screenshots[%s]&#39; % name] = element

    if self.session:
        params[&#39;session&#39;] = self.session

    if self.debug is True:
        params[&#39;debug&#39;] = self._bool_to_http(self.debug)

    if self.cache_clear is True:
        params[&#39;cache_clear&#39;] = self._bool_to_http(self.cache_clear)

    if self.cache_ttl is not None:
        params[&#39;cache_ttl&#39;] = self.cache_ttl

    if self.graphql:
        params[&#39;graphql&#39;] = self.graphql

    if self.js:
        params[&#39;js&#39;] = b64encode(self.js.encode(&#39;utf-8&#39;)).decode(&#39;utf-8&#39;)

    if self.rendering_wait:
        params[&#39;rendering_wait&#39;] = self.rendering_wait

    return params</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="scrapfly.scrape_config.ScrapeConfigError"><code class="flex name class">
<span>class <span class="ident">ScrapeConfigError</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Common base class for all non-exit exceptions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScrapeConfigError(Exception):
    pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="scrapfly" href="index.html">scrapfly</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="scrapfly.scrape_config.ScrapeConfig" href="#scrapfly.scrape_config.ScrapeConfig">ScrapeConfig</a></code></h4>
<ul class="">
<li><code><a title="scrapfly.scrape_config.ScrapeConfig.generate_distributed_correlation_id" href="#scrapfly.scrape_config.ScrapeConfig.generate_distributed_correlation_id">generate_distributed_correlation_id</a></code></li>
<li><code><a title="scrapfly.scrape_config.ScrapeConfig.to_api_params" href="#scrapfly.scrape_config.ScrapeConfig.to_api_params">to_api_params</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="scrapfly.scrape_config.ScrapeConfigError" href="#scrapfly.scrape_config.ScrapeConfigError">ScrapeConfigError</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>